{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcripción de voz a texto en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyaudio in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (0.2.14)\n",
      "Requirement already satisfied: sounddevice in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: webrtcvad in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (2.0.10)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\envs\\melo\\lib\\site-packages (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "# python version == 3.8.20\n",
    "\n",
    "!pip install pyaudio\n",
    "!pip install sounddevice\n",
    "!pip install torch\n",
    "!pip install webrtcvad\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\melo\\lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivos de audio disponibles:\n",
      "ID: 0, Nombre: Asignador de sonido Microsoft - Input, Entrada: 2, Salida: 0\n",
      "ID: 1, Nombre: Microphone (Realtek(R) Audio), Entrada: 2, Salida: 0\n",
      "ID: 2, Nombre: Asignador de sonido Microsoft - Output, Entrada: 0, Salida: 2\n",
      "ID: 3, Nombre: Headphones (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 4, Nombre: Acer V246HQL (HD Audio Driver f, Entrada: 0, Salida: 2\n",
      "ID: 5, Nombre: Speakers (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 6, Nombre: Controlador primario de captura de sonido, Entrada: 2, Salida: 0\n",
      "ID: 7, Nombre: Microphone (Realtek(R) Audio), Entrada: 2, Salida: 0\n",
      "ID: 8, Nombre: Controlador primario de sonido, Entrada: 0, Salida: 2\n",
      "ID: 9, Nombre: Headphones (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 10, Nombre: Acer V246HQL (HD Audio Driver for Display Audio), Entrada: 0, Salida: 2\n",
      "ID: 11, Nombre: Speakers (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 12, Nombre: Headphones (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 13, Nombre: Acer V246HQL (HD Audio Driver for Display Audio), Entrada: 0, Salida: 2\n",
      "ID: 14, Nombre: Speakers (Realtek(R) Audio), Entrada: 0, Salida: 2\n",
      "ID: 15, Nombre: Microphone (Realtek(R) Audio), Entrada: 2, Salida: 0\n",
      "ID: 16, Nombre: Input (), Entrada: 2, Salida: 0\n",
      "ID: 17, Nombre: Headphones (), Entrada: 0, Salida: 2\n",
      "ID: 18, Nombre: Headphones (), Entrada: 0, Salida: 2\n",
      "ID: 19, Nombre: Stereo Mix (Realtek HD Audio Stereo input), Entrada: 2, Salida: 0\n",
      "ID: 20, Nombre: Speakers 1 (Realtek HDA Primary output with SST), Entrada: 0, Salida: 2\n",
      "ID: 21, Nombre: Speakers 2 (Realtek HDA Primary output with SST), Entrada: 0, Salida: 2\n",
      "ID: 22, Nombre: PC Speaker (Realtek HDA Primary output with SST), Entrada: 2, Salida: 0\n",
      "ID: 23, Nombre: Microphone (Realtek HD Audio Mic input), Entrada: 2, Salida: 0\n",
      "ID: 24, Nombre: Headphones 1 (Realtek HD Audio 2nd output with SST), Entrada: 0, Salida: 2\n",
      "ID: 25, Nombre: Headphones 2 (Realtek HD Audio 2nd output with SST), Entrada: 0, Salida: 2\n",
      "ID: 26, Nombre: PC Speaker (Realtek HD Audio 2nd output with SST), Entrada: 2, Salida: 0\n",
      "ID: 27, Nombre: Acer V246HQL (ACX HD Audio Speaker), Entrada: 0, Salida: 2\n",
      "ID: 28, Nombre: Input (), Entrada: 2, Salida: 0\n",
      "ID: 29, Nombre: Headphones (), Entrada: 0, Salida: 2\n",
      "ID: 30, Nombre: Headphones (), Entrada: 0, Salida: 2\n",
      "ID: 31, Nombre: Headphones (), Entrada: 0, Salida: 2\n",
      "ID: 32, Nombre: Headphones (), Entrada: 0, Salida: 2\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import whisper\n",
    "import queue\n",
    "import pyaudio\n",
    "import wave\n",
    "import webrtcvad\n",
    "\n",
    "# Configuración de PyAudio\n",
    "#CHUNK = 4096    #1024              # Tamaño de cada fragmento de audio\n",
    "FORMAT = pyaudio.paInt16  # Formato de audio\n",
    "CHANNELS = 1              # Audio mono\n",
    "RATE = 16000 #11025              # Frecuencia de muestreo compatible con Whisper\n",
    "DURATION = 0.03\n",
    "BYTES_PRE_SAMPLE = 1\n",
    "chunk = DURATION * RATE  \n",
    "chunk = int(chunk)\n",
    "# Inicializar PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "# Inicializar cola para audio\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Crear el modelo de Whisper\n",
    "model = whisper.load_model(\"base\")  # Cambiar el modelo según sea necesario\n",
    "\n",
    "# Inicializar VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)  # Nivel de sensibilidad: 0 (menos sensible) a 3 (más sensible)\n",
    "\n",
    "# Función para detectar actividad de voz\n",
    "def is_speech(data, sample_rate):\n",
    "#    print(\"checking if is speech...\")\n",
    "    return vad.is_speech(data, sample_rate)\n",
    "\n",
    "# Función para reproducir audio\n",
    "def play_audio(data):\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, output=True)\n",
    "    stream.write(data)\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "# Función para capturar audio en tiempo real\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\")\n",
    "    audio_queue.put(indata.copy())\n",
    "\n",
    "print(\"Dispositivos de audio disponibles:\")\n",
    "for i in range(audio.get_device_count()):\n",
    "    info = audio.get_device_info_by_index(i)\n",
    "    print(f\"ID: {i}, Nombre: {info['name']}, Entrada: {info['maxInputChannels']}, Salida: {info['maxOutputChannels']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabando... Presiona Ctrl+C para detener.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\whisper\\lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Transcripción:  \n",
      "Grabación detenida.\n"
     ]
    }
   ],
   "source": [
    "# Opcion 1.- Real-Time con sounddevice\n",
    "with sd.InputStream(samplerate=RATE, channels=CHANNELS, callback=audio_callback):\n",
    "    print(\"Grabando... Presiona Ctrl+C para detener.\")\n",
    "    try:\n",
    "        # Acumular audio y transcribir en tiempo real\n",
    "        while True:\n",
    "            audio_data = audio_queue.get()\n",
    "            # Convertir audio a un formato que Whisper entienda\n",
    "            audio_np = np.squeeze(audio_data)\n",
    "            result = model.transcribe(audio_np, language=\"es\")\n",
    "            print(\"Transcripción: \", result[\"text\"])\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Grabación detenida.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabando... Presiona Ctrl+C para detener.\n",
      "Transcripción:   esto es porque se queda abierto\n",
      "Transcripción:  \n",
      "Grabación detenida.\n"
     ]
    }
   ],
   "source": [
    "# Opcion 2.- Real-Time con pyaudio\n",
    "# transcripción contínua periódica\n",
    "# Abrir el stream para capturar el audio del micrófono\n",
    "stream = audio.open(format=FORMAT, \n",
    "                    channels=CHANNELS, \n",
    "                    rate=RATE, \n",
    "                    input=True, \n",
    "                    frames_per_buffer=chunk)\n",
    "print(\"Grabando... Presiona Ctrl+C para detener.\")\n",
    "\n",
    "try:\n",
    "    audio_buffer = b''  # Buffer para almacenar audio capturado\n",
    "    while True:\n",
    "        # Leer audio desde el micrófono\n",
    "        data = stream.read(chunk, exception_on_overflow=False)\n",
    "        audio_buffer += data\n",
    "\n",
    "        # Procesar el audio cada 5 segundos aproximadamente\n",
    "        if len(audio_buffer) >= RATE * 5:\n",
    "            # Convertir a formato que Whisper entienda\n",
    "            audio_np = np.frombuffer(audio_buffer, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "            # Transcribir el audio\n",
    "            result = model.transcribe(audio_np, language=\"es\")\n",
    "            print(\"Transcripción: \", result[\"text\"])\n",
    "\n",
    "            # Reproducir el audio capturado\n",
    "            play_audio(audio_buffer)\n",
    "\n",
    "            # Limpiar el buffer después de procesar\n",
    "            audio_buffer = b''\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Grabación detenida.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Cerrar streams y PyAudio\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esperando actividad de voz... Presiona Ctrl+C para detener.\n",
      "Usuario comenzó a hablar...\n",
      "Usuario terminó de hablar.\n",
      "Transcribiendo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\melo\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción:   Hola, esto es un texto de prueba.\n",
      "Usuario comenzó a hablar...\n",
      "Usuario terminó de hablar.\n",
      "Transcribiendo...\n",
      "Transcripción:   Parece que funciona bien, ¿no?\n",
      "Grabación detenida.\n"
     ]
    }
   ],
   "source": [
    "# opcion 3.- transcripción mediante detección activa de voz VAD\n",
    "\n",
    "INPUT_DEVICE_ID = 1  # Cambiar según el dispositivo de entrada\n",
    "OUTPUT_DEVICE_ID = 3  # Cambiar según el dispositivo de salida\n",
    "\n",
    "# Abrir el stream para capturar el audio del micrófono\n",
    "stream = audio.open(format=FORMAT, \n",
    "                    channels=CHANNELS, \n",
    "                    rate=RATE, \n",
    "                    input=True, \n",
    "#                    output=False,\n",
    "#                    input_device_index=INPUT_DEVICE_ID,\n",
    "#                    output_device_index=OUTPUT_DEVICE_ID,\n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "print(\"Esperando actividad de voz... Presiona Ctrl+C para detener.\")\n",
    "\n",
    "try:\n",
    "    audio_buffer = b''  # Buffer para almacenar audio capturado\n",
    "    speaking = False    # Indicador de actividad de voz\n",
    "    while True:\n",
    "        # Leer audio desde el micrófono\n",
    "        data = stream.read(chunk, exception_on_overflow=False)\n",
    "        \n",
    "        # Detectar actividad de voz\n",
    "        if is_speech(data, RATE):\n",
    "            if not speaking:\n",
    "                print(\"Usuario comenzó a hablar...\")\n",
    "                speaking = True\n",
    "            audio_buffer += data\n",
    "        else:\n",
    "            if speaking:\n",
    "                print(\"Usuario terminó de hablar.\")\n",
    "                speaking = False\n",
    "                \n",
    "                # Convertir el audio capturado a formato que Whisper entienda\n",
    "                audio_np = np.frombuffer(audio_buffer, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "                \n",
    "                # Transcribir el audio\n",
    "                print(\"Transcribiendo...\")\n",
    "                result = model.transcribe(audio_np, language=\"es\")\n",
    "                print(\"Transcripción: \", result[\"text\"])\n",
    "                \n",
    "                # Limpiar el buffer después de procesar\n",
    "                audio_buffer = b''\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Grabación detenida.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Cerrar streams y PyAudio\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "melo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
